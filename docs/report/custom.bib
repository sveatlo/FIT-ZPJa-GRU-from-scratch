@misc{cho2014learning,
	title={Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}, 
	author={Kyunghyun Cho and Bart van Merrienboer and Caglar Gulcehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio},
	year={2014},
	eprint={1406.1078},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@misc{dey2017gatevariants,
	title={Gate-Variants of Gated Recurrent Unit (GRU) Neural Networks}, 
	author={Rahul Dey and Fathi M. Salem},
	year={2017},
	eprint={1701.05923},
	archivePrefix={arXiv},
	primaryClass={cs.NE}
}
@article{li2016tutorial,
	title={A Tutorial On Backward Propagation Through Time (BPTT) In The Gated Recurrent Unit (GRU) RNN},
	author={Li, Minchen},
	year={2016}
}

@misc{kingma2017adam,
	title={Adam: A Method for Stochastic Optimization}, 
	author={Diederik P. Kingma and Jimmy Ba},
	year={2017},
	eprint={1412.6980},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}
@article{duchi2011adaptive,
	title={Adaptive subgradient methods for online learning and stochastic optimization.},
	author={Duchi, John and Hazan, Elad and Singer, Yoram},
	journal={Journal of machine learning research},
	volume={12},
	number={7},
	year={2011}
}
















@inproceedings{fajcik2021r2d2,
	title = "{R2-D2}: A Modular Baseline for Open-Domain Question Answering",
	author = "Fajcik, Martin  and
	Docekal, Martin  and
	Ondrej, Karel  and
	Smrz, Pavel",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
	month = nov,
	year = "2021",
	address = "Punta Cana, Dominican Republic",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.findings-emnlp.73",
	pages = "854--870",
	abstract = "This work presents a novel four-stage open-domain QA pipeline R2-D2 (Rank twice, reaD twice). The pipeline is composed of a retriever, passage reranker, extractive reader, generative reader and a mechanism that aggregates the final prediction from all system{'}s components. We demonstrate its strength across three open-domain QA datasets: NaturalQuestions, TriviaQA and EfficientQA, surpassing state-of-the-art on the first two. Our analysis demonstrates that: (i) combining extractive and generative reader yields absolute improvements up to 5 exact match and it is at least twice as effective as the posterior averaging ensemble of the same models with different parameters, (ii) the extractive reader with fewer parameters can match the performance of the generative reader on extractive QA datasets.",
}

@inproceedings{karpukhin2020dense,
	address={Online},
	author={Karpukhin, Vladimir  and
	Oguz, Barlas  and
	Min, Sewon  and
	Lewis, Patrick  and
	Wu, Ledell  and
	Edunov, Sergey  and
	Chen, Danqi  and
	Yih, Wen-tau},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	doi = {10.18653/v1/2020.emnlp-main.550},
	pages = {6769--6781},
	publisher = {Association for Computational Linguistics},
	title = {Dense Passage Retrieval for Open-Domain Question Answering},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.550},
	year = {2020}
}

@article{nogueira2019passage,
	author = {Nogueira, Rodrigo and Cho, Kyunghyun},
	journal = {arXiv preprint arXiv:1901.04085},
	title = {Passage Re-ranking with BERT},
	year = {2019}
}

@article{luan2020sparse,
	author = {Yi Luan and Jacob Eisenstein and Kristina Toutanova and Michael Collins},
	journal = {Transactions of the Association for Computational Linguistics},
	publisher = {MIT Press - Journals},
	title = {Sparse, Dense, and Attentional Representations for Text Retrieval},
	year = {2021}
}